# ðŸŒ§ï¸ Rain-Tomorrow Prediction â€“ Australia Weather

Accurately predicting whether it will rain tomorrow can help farmers, commuters, and emergency services make better decisions.
This repository contains the full **data-mining pipeline** I built for Assignment 3 of *31250 Introduction to Data Analytics* at UTS. It moves from raw Bureau-of-Meteorology data through to a **Soft-Voting ensemble** that achieved a public **Kaggle score of 0 Â· 81126**.

---

## Table of Contents

1. [Problem Statement](#problem-statement)
2. [Dataset](#dataset)
3. [End-to-End Approach](#end-to-end-approach)
4. [Why Each Choice?](#why-each-choice)
5. [Experimental Results](#experimental-results)
6. [Future Work](#future-work)

---

## Problem Statement

Predict the binary target **`RainTomorrow`** (Yes / No) for the next day using \~10 years of daily weather observations collected across Australia (51 199 rows, 22 columns).

---

## Dataset

* **16 numerical** + **5 categorical** predictors (wind directions, etc.)
* Pronounced **class imbalance** (â‰ˆ22 % â€œRainâ€).
* Up to **47 % missingness** in several sensor-based columns (e.g. *Sunshine*).

---

## End-to-End Approach

| Stage                        | Key Techniques                                                                            |
| ---------------------------- | ----------------------------------------------------------------------------------------- |
| **Exploration**              | Descriptive stats, correlation heat-map.                                                  |
| **Split**                    | 90 / 10 trainâ€“test *before* any transforms to avoid leakage.                              |
| **Missing-Value Imputation** | *MICE* (IterativeImputer) for numerics; **mode** for categoricals.                        |
| **Outliers**                 | 1.5 Ã— IQR detection âžœ **Winsorize** at 5áµ—Ê° & 95áµ—Ê° percentiles.                            |
| **Encoding**                 | **Label Encoding** (after benchmarking vs One-Hot & Binary).                              |
| **Class Imbalance**          | **Bootstrapped up-sampling** of the minority class *only on train*.                       |
| **Scaling**                  | **RobustScaler** for outlier-heavy cols; **StandardScaler** for the rest.                 |
| **Feature Selection**        | **RFE** with Random Forest â†’ top 15 features.                                             |
| **Modeling**                 | Baseline grid of 6 algorithms â†’ tune **RF, XGBoost, MLP** with **HalvingRandomSearchCV**. |
| **Ensembling**               | Soft-Voting (weights 4 : 1 : 2 for XGB : MLP : RF).                                       |
| **Evaluation**               | Stratified 5-fold CV on F1; full test-set metrics; Kaggle submission.                     |

---

## Why Each Choice?

* **MICE > Mean/Median/KNN** â€“ preserves multivariate relationships âžœ +F1.
* **Bootstrapped up-sampling** beat SMOTE / SMOTE-NC by giving cleaner decision boundaries while preventing synthetic noise.
* **HalvingRandomSearchCV** explored wider hyper-parameter space with 75 % less computation than exhaustive GridSearch.
* **Soft Voting** captured XGBoostâ€™s precision on class 0, MLPâ€™s recall on class 1, and RFâ€™s overall balance.

---

## Experimental Results

| Model                     | Class 1 Precision | Class 1 Recall | Class 1 F1  | ROC-AUC     |
| ------------------------- | ----------------- | -------------- | ----------- | ----------- |
| **Random Forest (tuned)** | **0 Â· 71**        | 0 Â· 56         | 0 Â· 625     | 0 Â· 875     |
| **XGBoost (tuned)**       | 0 Â· 56            | 0 Â· 76         | 0 Â· 642     | **0 Â· 881** |
| **MLP (tuned)**           | 0 Â· 50            | **0 Â· 79**     | 0 Â· 611     | 0 Â· 868     |
| **Soft-Voting Ensemble**  | 0 Â· 63            | 0 Â· 70         | **0 Â· 721** | 0 Â· 883     |

*Public leaderboard*: **0 Â· 81126** F1.
*Confusion-matrix & ROC plots* can be found in the notebook.

Future Work
Cost-sensitive loss or focal loss to push minority-class F1 higher.

Try temporal models (Temporal Fusion Transformer, LSTM) to exploit seasonality.

Deploy as a REST API (FastAPI + Docker) and automate a daily forecast pipeline.

